{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the extractor and feeder file from Github and copy them in your following Anaconda folder:\n",
    "\n",
    "C:\\Users\\bandi\\Anaconda3\\Lib\\site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def parse_page(x):\n",
    "    x = x.split('_')\n",
    "    return ' '.join(x[:-3]), x[-3], x[-2], x[-1]\n",
    "\n",
    "\n",
    "def nan_fill_forward(x):\n",
    "    for i in range(x.shape[0]):\n",
    "        fill_val = None\n",
    "        for j in range(x.shape[1] - 3, x.shape[1]):\n",
    "            if np.isnan(x[i, j]) and fill_val is not None:\n",
    "                x[i, j] = fill_val\n",
    "            else:\n",
    "                fill_val = x[i, j]\n",
    "    return x\n",
    "\n",
    "\n",
    "df = pd.read_csv('train_2.csv', encoding='utf-8')\n",
    "date_cols = [i for i in df.columns if i != 'Page']\n",
    "\n",
    "df['name'], df['project'], df['access'], df['agent'] = zip(*df['Page'].apply(parse_page))\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['project'] = le.fit_transform(df['project'])\n",
    "df['access'] = le.fit_transform(df['access'])\n",
    "df['agent'] = le.fit_transform(df['agent'])\n",
    "df['page_id'] = le.fit_transform(df['Page'])\n",
    "\n",
    "if not os.path.isdir('data/processed'):\n",
    "    os.makedirs('data/processed')\n",
    "\n",
    "df[['page_id', 'Page']].to_csv('data/processed/page_ids.csv', encoding='utf-8', index=False)\n",
    "\n",
    "data = df[date_cols].values\n",
    "np.save('data/processed/data.npy', np.nan_to_num(data))\n",
    "np.save('data/processed/is_nan.npy', np.isnan(data).astype(int))\n",
    "np.save('data/processed/project.npy', df['project'].values)\n",
    "np.save('data/processed/access.npy', df['access'].values)\n",
    "np.save('data/processed/agent.npy', df['agent'].values)\n",
    "np.save('data/processed/page_id.npy', df['page_id'].values)\n",
    "\n",
    "test_data = nan_fill_forward(df[date_cols].values)\n",
    "np.save('data/processed/test_data.npy', np.nan_to_num(test_data))\n",
    "np.save('data/processed/test_is_nan.npy', np.isnan(test_data).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "\n",
      "new run with parameters:\n",
      "{'batch_size': 128,\n",
      " 'checkpoint_dir': './checkpoints',\n",
      " 'dilations': [1,\n",
      "               2,\n",
      "               4,\n",
      "               8,\n",
      "               16,\n",
      "               32,\n",
      "               64,\n",
      "               128,\n",
      "               1,\n",
      "               2,\n",
      "               4,\n",
      "               8,\n",
      "               16,\n",
      "               32,\n",
      "               64,\n",
      "               128,\n",
      "               1,\n",
      "               2,\n",
      "               4,\n",
      "               8,\n",
      "               16,\n",
      "               32,\n",
      "               64,\n",
      "               128],\n",
      " 'early_stopping_steps': 5000,\n",
      " 'enable_parameter_averaging': False,\n",
      " 'filter_widths': [2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2,\n",
      "                   2],\n",
      " 'grad_clip': 20,\n",
      " 'keep_prob_scalar': 1.0,\n",
      " 'learning_rate': 0.001,\n",
      " 'log_dir': './logs',\n",
      " 'log_interval': 10,\n",
      " 'loss_averaging_window': 100,\n",
      " 'min_steps_to_checkpoint': 500,\n",
      " 'num_decode_steps': 64,\n",
      " 'num_restarts': 2,\n",
      " 'num_training_steps': 200000,\n",
      " 'num_validation_batches': 1,\n",
      " 'optimizer': 'adam',\n",
      " 'prediction_dir': './predictions',\n",
      " 'reader': <__main__.DataReader object at 0x7fc8211a0dd8>,\n",
      " 'regularization_constant': 0.0,\n",
      " 'residual_channels': 32,\n",
      " 'skip_channels': 32,\n",
      " 'warm_start_init_step': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 137809\n",
      "val size 7254\n",
      "test size 145063\n",
      "WARNING:tensorflow:From /home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "all parameters:\n",
      "[('Variable:0', []),\n",
      " ('Variable_1:0', []),\n",
      " ('x-proj-encode/weights:0', [18, 32]),\n",
      " ('x-proj-encode/biases:0', [32]),\n",
      " ('dilated-conv-encode-0/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-0/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-0/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-0/biases:0', [64]),\n",
      " ('dilated-conv-encode-1/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-1/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-1/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-1/biases:0', [64]),\n",
      " ('dilated-conv-encode-2/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-2/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-2/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-2/biases:0', [64]),\n",
      " ('dilated-conv-encode-3/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-3/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-3/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-3/biases:0', [64]),\n",
      " ('dilated-conv-encode-4/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-4/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-4/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-4/biases:0', [64]),\n",
      " ('dilated-conv-encode-5/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-5/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-5/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-5/biases:0', [64]),\n",
      " ('dilated-conv-encode-6/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-6/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-6/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-6/biases:0', [64]),\n",
      " ('dilated-conv-encode-7/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-7/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-7/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-7/biases:0', [64]),\n",
      " ('dilated-conv-encode-8/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-8/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-8/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-8/biases:0', [64]),\n",
      " ('dilated-conv-encode-9/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-9/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-9/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-9/biases:0', [64]),\n",
      " ('dilated-conv-encode-10/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-10/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-10/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-10/biases:0', [64]),\n",
      " ('dilated-conv-encode-11/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-11/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-11/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-11/biases:0', [64]),\n",
      " ('dilated-conv-encode-12/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-12/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-12/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-12/biases:0', [64]),\n",
      " ('dilated-conv-encode-13/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-13/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-13/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-13/biases:0', [64]),\n",
      " ('dilated-conv-encode-14/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-14/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-14/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-14/biases:0', [64]),\n",
      " ('dilated-conv-encode-15/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-15/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-15/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-15/biases:0', [64]),\n",
      " ('dilated-conv-encode-16/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-16/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-16/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-16/biases:0', [64]),\n",
      " ('dilated-conv-encode-17/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-17/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-17/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-17/biases:0', [64]),\n",
      " ('dilated-conv-encode-18/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-18/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-18/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-18/biases:0', [64]),\n",
      " ('dilated-conv-encode-19/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-19/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-19/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-19/biases:0', [64]),\n",
      " ('dilated-conv-encode-20/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-20/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-20/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-20/biases:0', [64]),\n",
      " ('dilated-conv-encode-21/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-21/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-21/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-21/biases:0', [64]),\n",
      " ('dilated-conv-encode-22/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-22/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-22/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-22/biases:0', [64]),\n",
      " ('dilated-conv-encode-23/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-23/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-23/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-23/biases:0', [64]),\n",
      " ('dense-encode-1/weights:0', [768, 128]),\n",
      " ('dense-encode-1/biases:0', [128]),\n",
      " ('dense-encode-2/weights:0', [128, 1]),\n",
      " ('dense-encode-2/biases:0', [1]),\n",
      " ('x-proj-decode/weights:0', [80, 32]),\n",
      " ('x-proj-decode/biases:0', [32]),\n",
      " ('dilated-conv-decode-0/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-0/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-0/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-0/biases:0', [64]),\n",
      " ('dilated-conv-decode-1/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-1/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-1/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-1/biases:0', [64]),\n",
      " ('dilated-conv-decode-2/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-2/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-2/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-2/biases:0', [64]),\n",
      " ('dilated-conv-decode-3/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-3/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-3/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-3/biases:0', [64]),\n",
      " ('dilated-conv-decode-4/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-4/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-4/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-4/biases:0', [64]),\n",
      " ('dilated-conv-decode-5/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-5/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-5/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-5/biases:0', [64]),\n",
      " ('dilated-conv-decode-6/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-6/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-6/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-6/biases:0', [64]),\n",
      " ('dilated-conv-decode-7/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-7/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-7/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-7/biases:0', [64]),\n",
      " ('dilated-conv-decode-8/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-8/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-8/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-8/biases:0', [64]),\n",
      " ('dilated-conv-decode-9/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-9/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-9/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-9/biases:0', [64]),\n",
      " ('dilated-conv-decode-10/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-10/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-10/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-10/biases:0', [64]),\n",
      " ('dilated-conv-decode-11/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-11/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-11/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-11/biases:0', [64]),\n",
      " ('dilated-conv-decode-12/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-12/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-12/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-12/biases:0', [64]),\n",
      " ('dilated-conv-decode-13/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-13/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-13/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-13/biases:0', [64]),\n",
      " ('dilated-conv-decode-14/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-14/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-14/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-14/biases:0', [64]),\n",
      " ('dilated-conv-decode-15/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-15/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-15/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-15/biases:0', [64]),\n",
      " ('dilated-conv-decode-16/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-16/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-16/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-16/biases:0', [64]),\n",
      " ('dilated-conv-decode-17/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-17/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-17/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-17/biases:0', [64]),\n",
      " ('dilated-conv-decode-18/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-18/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-18/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-18/biases:0', [64]),\n",
      " ('dilated-conv-decode-19/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-19/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-19/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-19/biases:0', [64]),\n",
      " ('dilated-conv-decode-20/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-20/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-20/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-20/biases:0', [64]),\n",
      " ('dilated-conv-decode-21/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-21/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-21/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-21/biases:0', [64]),\n",
      " ('dilated-conv-decode-22/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-22/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-22/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-22/biases:0', [64]),\n",
      " ('dilated-conv-decode-23/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-23/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-23/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-23/biases:0', [64]),\n",
      " ('dense-decode-1/weights:0', [768, 128]),\n",
      " ('dense-decode-1/biases:0', [128]),\n",
      " ('dense-decode-2/weights:0', [128, 1]),\n",
      " ('dense-decode-2/biases:0', [1]),\n",
      " ('beta1_power:0', []),\n",
      " ('beta2_power:0', []),\n",
      " ('x-proj-encode/weights/Adam:0', [18, 32]),\n",
      " ('x-proj-encode/weights/Adam_1:0', [18, 32]),\n",
      " ('x-proj-encode/biases/Adam:0', [32]),\n",
      " ('x-proj-encode/biases/Adam_1:0', [32]),\n",
      " ('dilated-conv-encode-0/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-0/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-0/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-0/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-0/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-0/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-0/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-0/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-1/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-1/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-1/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-1/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-1/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-1/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-1/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-1/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-2/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-2/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-2/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-2/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-2/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-2/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-2/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-2/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-3/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-3/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-3/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-3/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-3/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-3/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-3/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-3/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-4/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-4/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-4/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-4/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-4/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-4/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-4/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-4/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-5/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-5/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-5/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-5/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-5/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-5/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-5/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-5/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-6/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-6/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-6/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-6/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-6/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-6/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-6/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-6/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-7/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-7/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-7/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-7/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-7/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-7/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-7/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-7/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-8/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-8/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-8/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-8/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-8/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-8/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-8/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-8/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-9/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-9/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-9/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-9/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-9/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-9/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-9/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-9/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-10/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-10/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-10/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-10/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-10/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-10/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-10/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-10/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-11/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-11/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-11/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-11/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-11/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-11/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-11/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-11/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-12/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-12/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-12/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-12/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-12/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-12/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-12/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-12/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-13/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-13/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-13/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-13/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-13/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-13/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-13/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-13/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-14/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-14/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-14/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-14/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-14/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-14/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-14/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-14/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-15/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-15/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-15/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-15/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-15/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-15/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-15/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-15/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-16/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-16/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-16/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-16/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-16/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-16/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-16/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-16/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-17/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-17/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-17/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-17/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-17/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-17/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-17/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-17/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-18/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-18/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-18/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-18/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-18/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-18/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-18/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-18/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-19/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-19/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-19/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-19/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-19/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-19/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-19/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-19/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-20/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-20/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-20/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-20/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-20/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-20/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-20/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-20/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-21/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-21/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-21/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-21/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-21/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-21/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-21/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-21/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-22/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-22/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-22/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-22/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-22/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-22/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-22/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-22/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-encode-23/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-23/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-23/biases/Adam:0', [64]),\n",
      " ('dilated-conv-encode-23/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-encode-23/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-23/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-23/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-encode-23/biases/Adam_1:0', [64]),\n",
      " ('dense-encode-1/weights/Adam:0', [768, 128]),\n",
      " ('dense-encode-1/weights/Adam_1:0', [768, 128]),\n",
      " ('dense-encode-1/biases/Adam:0', [128]),\n",
      " ('dense-encode-1/biases/Adam_1:0', [128]),\n",
      " ('dense-encode-2/weights/Adam:0', [128, 1]),\n",
      " ('dense-encode-2/weights/Adam_1:0', [128, 1]),\n",
      " ('dense-encode-2/biases/Adam:0', [1]),\n",
      " ('dense-encode-2/biases/Adam_1:0', [1]),\n",
      " ('x-proj-decode/weights/Adam:0', [80, 32]),\n",
      " ('x-proj-decode/weights/Adam_1:0', [80, 32]),\n",
      " ('x-proj-decode/biases/Adam:0', [32]),\n",
      " ('x-proj-decode/biases/Adam_1:0', [32]),\n",
      " ('dilated-conv-decode-0/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-0/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-0/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-0/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-0/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-0/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-0/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-0/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-1/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-1/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-1/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-1/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-1/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-1/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-1/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-1/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-2/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-2/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-2/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-2/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-2/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-2/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-2/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-2/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-3/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-3/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-3/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-3/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-3/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-3/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-3/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-3/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-4/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-4/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-4/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-4/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-4/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-4/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-4/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-4/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-5/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-5/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-5/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-5/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-5/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-5/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-5/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-5/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-6/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-6/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-6/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-6/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-6/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-6/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-6/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-6/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-7/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-7/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-7/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-7/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-7/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-7/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-7/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-7/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-8/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-8/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-8/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-8/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-8/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-8/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-8/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-8/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-9/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-9/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-9/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-9/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-9/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-9/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-9/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-9/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-10/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-10/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-10/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-10/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-10/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-10/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-10/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-10/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-11/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-11/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-11/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-11/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-11/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-11/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-11/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-11/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-12/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-12/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-12/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-12/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-12/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-12/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-12/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-12/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-13/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-13/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-13/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-13/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-13/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-13/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-13/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-13/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-14/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-14/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-14/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-14/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-14/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-14/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-14/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-14/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-15/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-15/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-15/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-15/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-15/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-15/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-15/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-15/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-16/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-16/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-16/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-16/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-16/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-16/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-16/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-16/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-17/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-17/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-17/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-17/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-17/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-17/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-17/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-17/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-18/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-18/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-18/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-18/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-18/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-18/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-18/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-18/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-19/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-19/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-19/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-19/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-19/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-19/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-19/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-19/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-20/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-20/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-20/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-20/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-20/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-20/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-20/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-20/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-21/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-21/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-21/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-21/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-21/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-21/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-21/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-21/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-22/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-22/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-22/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-22/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-22/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-22/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-22/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-22/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-decode-23/weights/Adam:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-23/weights/Adam_1:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-23/biases/Adam:0', [64]),\n",
      " ('dilated-conv-decode-23/biases/Adam_1:0', [64]),\n",
      " ('dilated-conv-proj-decode-23/weights/Adam:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-23/weights/Adam_1:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-23/biases/Adam:0', [64]),\n",
      " ('dilated-conv-proj-decode-23/biases/Adam_1:0', [64]),\n",
      " ('dense-decode-1/weights/Adam:0', [768, 128]),\n",
      " ('dense-decode-1/weights/Adam_1:0', [768, 128]),\n",
      " ('dense-decode-1/biases/Adam:0', [128]),\n",
      " ('dense-decode-1/biases/Adam_1:0', [128]),\n",
      " ('dense-decode-2/weights/Adam:0', [128, 1]),\n",
      " ('dense-decode-2/weights/Adam_1:0', [128, 1]),\n",
      " ('dense-decode-2/biases/Adam:0', [1]),\n",
      " ('dense-decode-2/biases/Adam_1:0', [1])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainable parameters:\n",
      "[('x-proj-encode/weights:0', [18, 32]),\n",
      " ('x-proj-encode/biases:0', [32]),\n",
      " ('dilated-conv-encode-0/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-0/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-0/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-0/biases:0', [64]),\n",
      " ('dilated-conv-encode-1/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-1/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-1/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-1/biases:0', [64]),\n",
      " ('dilated-conv-encode-2/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-2/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-2/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-2/biases:0', [64]),\n",
      " ('dilated-conv-encode-3/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-3/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-3/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-3/biases:0', [64]),\n",
      " ('dilated-conv-encode-4/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-4/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-4/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-4/biases:0', [64]),\n",
      " ('dilated-conv-encode-5/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-5/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-5/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-5/biases:0', [64]),\n",
      " ('dilated-conv-encode-6/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-6/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-6/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-6/biases:0', [64]),\n",
      " ('dilated-conv-encode-7/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-7/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-7/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-7/biases:0', [64]),\n",
      " ('dilated-conv-encode-8/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-8/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-8/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-8/biases:0', [64]),\n",
      " ('dilated-conv-encode-9/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-9/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-9/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-9/biases:0', [64]),\n",
      " ('dilated-conv-encode-10/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-10/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-10/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-10/biases:0', [64]),\n",
      " ('dilated-conv-encode-11/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-11/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-11/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-11/biases:0', [64]),\n",
      " ('dilated-conv-encode-12/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-12/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-12/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-12/biases:0', [64]),\n",
      " ('dilated-conv-encode-13/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-13/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-13/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-13/biases:0', [64]),\n",
      " ('dilated-conv-encode-14/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-14/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-14/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-14/biases:0', [64]),\n",
      " ('dilated-conv-encode-15/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-15/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-15/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-15/biases:0', [64]),\n",
      " ('dilated-conv-encode-16/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-16/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-16/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-16/biases:0', [64]),\n",
      " ('dilated-conv-encode-17/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-17/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-17/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-17/biases:0', [64]),\n",
      " ('dilated-conv-encode-18/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-18/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-18/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-18/biases:0', [64]),\n",
      " ('dilated-conv-encode-19/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-19/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-19/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-19/biases:0', [64]),\n",
      " ('dilated-conv-encode-20/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-20/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-20/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-20/biases:0', [64]),\n",
      " ('dilated-conv-encode-21/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-21/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-21/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-21/biases:0', [64]),\n",
      " ('dilated-conv-encode-22/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-22/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-22/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-22/biases:0', [64]),\n",
      " ('dilated-conv-encode-23/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-encode-23/biases:0', [64]),\n",
      " ('dilated-conv-proj-encode-23/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-encode-23/biases:0', [64]),\n",
      " ('dense-encode-1/weights:0', [768, 128]),\n",
      " ('dense-encode-1/biases:0', [128]),\n",
      " ('dense-encode-2/weights:0', [128, 1]),\n",
      " ('dense-encode-2/biases:0', [1]),\n",
      " ('x-proj-decode/weights:0', [80, 32]),\n",
      " ('x-proj-decode/biases:0', [32]),\n",
      " ('dilated-conv-decode-0/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-0/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-0/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-0/biases:0', [64]),\n",
      " ('dilated-conv-decode-1/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-1/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-1/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-1/biases:0', [64]),\n",
      " ('dilated-conv-decode-2/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-2/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-2/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-2/biases:0', [64]),\n",
      " ('dilated-conv-decode-3/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-3/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-3/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-3/biases:0', [64]),\n",
      " ('dilated-conv-decode-4/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-4/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-4/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-4/biases:0', [64]),\n",
      " ('dilated-conv-decode-5/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-5/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-5/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-5/biases:0', [64]),\n",
      " ('dilated-conv-decode-6/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-6/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-6/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-6/biases:0', [64]),\n",
      " ('dilated-conv-decode-7/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-7/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-7/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-7/biases:0', [64]),\n",
      " ('dilated-conv-decode-8/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-8/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-8/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-8/biases:0', [64]),\n",
      " ('dilated-conv-decode-9/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-9/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-9/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-9/biases:0', [64]),\n",
      " ('dilated-conv-decode-10/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-10/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-10/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-10/biases:0', [64]),\n",
      " ('dilated-conv-decode-11/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-11/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-11/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-11/biases:0', [64]),\n",
      " ('dilated-conv-decode-12/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-12/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-12/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-12/biases:0', [64]),\n",
      " ('dilated-conv-decode-13/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-13/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-13/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-13/biases:0', [64]),\n",
      " ('dilated-conv-decode-14/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-14/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-14/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-14/biases:0', [64]),\n",
      " ('dilated-conv-decode-15/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-15/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-15/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-15/biases:0', [64]),\n",
      " ('dilated-conv-decode-16/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-16/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-16/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-16/biases:0', [64]),\n",
      " ('dilated-conv-decode-17/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-17/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-17/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-17/biases:0', [64]),\n",
      " ('dilated-conv-decode-18/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-18/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-18/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-18/biases:0', [64]),\n",
      " ('dilated-conv-decode-19/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-19/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-19/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-19/biases:0', [64]),\n",
      " ('dilated-conv-decode-20/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-20/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-20/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-20/biases:0', [64]),\n",
      " ('dilated-conv-decode-21/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-21/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-21/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-21/biases:0', [64]),\n",
      " ('dilated-conv-decode-22/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-22/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-22/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-22/biases:0', [64]),\n",
      " ('dilated-conv-decode-23/weights:0', [2, 32, 64]),\n",
      " ('dilated-conv-decode-23/biases:0', [64]),\n",
      " ('dilated-conv-proj-decode-23/weights:0', [32, 64]),\n",
      " ('dilated-conv-proj-decode-23/biases:0', [64]),\n",
      " ('dense-decode-1/weights:0', [768, 128]),\n",
      " ('dense-decode-1/biases:0', [128]),\n",
      " ('dense-decode-2/weights:0', [128, 1]),\n",
      " ('dense-decode-2/biases:0', [1])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainable parameter count:\n",
      "501378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[step        0]]     [[train]]     loss: 0.55124801       [[val]]     loss: 0.62897426       \n",
      "[[step       10]]     [[train]]     loss: 0.58313724       [[val]]     loss: 0.60352946       \n",
      "[[step       20]]     [[train]]     loss: 0.57876486       [[val]]     loss: 0.58393752       \n",
      "[[step       30]]     [[train]]     loss: 0.56148133       [[val]]     loss: 0.57551065       \n",
      "[[step       40]]     [[train]]     loss: 0.54858191       [[val]]     loss: 0.56093422       \n",
      "[[step       50]]     [[train]]     loss: 0.53711307       [[val]]     loss: 0.5465588        \n",
      "[[step       60]]     [[train]]     loss: 0.52676796       [[val]]     loss: 0.53348851       \n",
      "[[step       70]]     [[train]]     loss: 0.51921872       [[val]]     loss: 0.5241979        \n",
      "[[step       80]]     [[train]]     loss: 0.51153092       [[val]]     loss: 0.51769502       \n",
      "[[step       90]]     [[train]]     loss: 0.50694635       [[val]]     loss: 0.51148621       \n",
      "[[step      100]]     [[train]]     loss: 0.50154027       [[val]]     loss: 0.50483174       \n",
      "[[step      110]]     [[train]]     loss: 0.48719533       [[val]]     loss: 0.48977672       \n",
      "[[step      120]]     [[train]]     loss: 0.47474141       [[val]]     loss: 0.47872811       \n",
      "[[step      130]]     [[train]]     loss: 0.4674329        [[val]]     loss: 0.46886397       \n",
      "[[step      140]]     [[train]]     loss: 0.46092405       [[val]]     loss: 0.46206439       \n",
      "[[step      150]]     [[train]]     loss: 0.45737816       [[val]]     loss: 0.4573425        \n",
      "[[step      160]]     [[train]]     loss: 0.45288258       [[val]]     loss: 0.45398482       \n",
      "[[step      170]]     [[train]]     loss: 0.44960326       [[val]]     loss: 0.45036827       \n",
      "[[step      180]]     [[train]]     loss: 0.44765273       [[val]]     loss: 0.44670459       \n",
      "[[step      190]]     [[train]]     loss: 0.44461092       [[val]]     loss: 0.44283072       \n",
      "[[step      200]]     [[train]]     loss: 0.44311206       [[val]]     loss: 0.44193892       \n",
      "[[step      210]]     [[train]]     loss: 0.44290013       [[val]]     loss: 0.44014142       \n",
      "[[step      220]]     [[train]]     loss: 0.4415908        [[val]]     loss: 0.43810212       \n",
      "[[step      230]]     [[train]]     loss: 0.43999212       [[val]]     loss: 0.43567161       \n",
      "[[step      240]]     [[train]]     loss: 0.43830709       [[val]]     loss: 0.4344333        \n",
      "[[step      250]]     [[train]]     loss: 0.43567747       [[val]]     loss: 0.43334518       \n",
      "[[step      260]]     [[train]]     loss: 0.43456335       [[val]]     loss: 0.4310867        \n",
      "[[step      270]]     [[train]]     loss: 0.43258239       [[val]]     loss: 0.43066994       \n",
      "[[step      280]]     [[train]]     loss: 0.43224593       [[val]]     loss: 0.42937482       \n",
      "[[step      290]]     [[train]]     loss: 0.43084861       [[val]]     loss: 0.42842432       \n",
      "[[step      300]]     [[train]]     loss: 0.4281533        [[val]]     loss: 0.42488794       \n",
      "[[step      310]]     [[train]]     loss: 0.42524454       [[val]]     loss: 0.42430109       \n",
      "[[step      320]]     [[train]]     loss: 0.42344054       [[val]]     loss: 0.42264061       \n",
      "[[step      330]]     [[train]]     loss: 0.42092269       [[val]]     loss: 0.42084818       \n",
      "[[step      340]]     [[train]]     loss: 0.42058408       [[val]]     loss: 0.41994337       \n",
      "[[step      350]]     [[train]]     loss: 0.41962589       [[val]]     loss: 0.41948687       \n",
      "[[step      360]]     [[train]]     loss: 0.41943577       [[val]]     loss: 0.41980192       \n",
      "[[step      370]]     [[train]]     loss: 0.4191187        [[val]]     loss: 0.41992969       \n",
      "[[step      380]]     [[train]]     loss: 0.4169225        [[val]]     loss: 0.41955752       \n",
      "[[step      390]]     [[train]]     loss: 0.41740947       [[val]]     loss: 0.42014773       \n",
      "[[step      400]]     [[train]]     loss: 0.41688499       [[val]]     loss: 0.42040575       \n",
      "[[step      410]]     [[train]]     loss: 0.41629197       [[val]]     loss: 0.42008776       \n",
      "[[step      420]]     [[train]]     loss: 0.41683627       [[val]]     loss: 0.42099248       \n",
      "[[step      430]]     [[train]]     loss: 0.41729334       [[val]]     loss: 0.42097006       \n",
      "[[step      440]]     [[train]]     loss: 0.41650452       [[val]]     loss: 0.41964597       \n",
      "[[step      450]]     [[train]]     loss: 0.4164243        [[val]]     loss: 0.41813956       \n",
      "[[step      460]]     [[train]]     loss: 0.41502111       [[val]]     loss: 0.41914164       \n",
      "[[step      470]]     [[train]]     loss: 0.41460419       [[val]]     loss: 0.41773169       \n",
      "[[step      480]]     [[train]]     loss: 0.41498548       [[val]]     loss: 0.41707749       \n",
      "[[step      490]]     [[train]]     loss: 0.41382395       [[val]]     loss: 0.41596402       \n",
      "[[step      500]]     [[train]]     loss: 0.41493097       [[val]]     loss: 0.41524925       \n",
      "[[step      510]]     [[train]]     loss: 0.41579115       [[val]]     loss: 0.41561505       \n",
      "[[step      520]]     [[train]]     loss: 0.41401509       [[val]]     loss: 0.41484867       \n",
      "creating checkpoint directory ./checkpoints\n",
      "saving model to ./checkpoints/model\n",
      "[[step      530]]     [[train]]     loss: 0.41457531       [[val]]     loss: 0.4150337        \n",
      "[[step      540]]     [[train]]     loss: 0.41363815       [[val]]     loss: 0.41604887       \n",
      "[[step      550]]     [[train]]     loss: 0.4138422        [[val]]     loss: 0.41626027       \n",
      "[[step      560]]     [[train]]     loss: 0.41641416       [[val]]     loss: 0.41539894       \n",
      "[[step      570]]     [[train]]     loss: 0.41430553       [[val]]     loss: 0.41403966       \n",
      "saving model to ./checkpoints/model\n",
      "[[step      580]]     [[train]]     loss: 0.41322311       [[val]]     loss: 0.41273532       \n",
      "saving model to ./checkpoints/model\n",
      "[[step      590]]     [[train]]     loss: 0.41248131       [[val]]     loss: 0.4132505        \n",
      "[[step      600]]     [[train]]     loss: 0.41152773       [[val]]     loss: 0.41349197       \n",
      "[[step      610]]     [[train]]     loss: 0.41264758       [[val]]     loss: 0.41075207       \n",
      "saving model to ./checkpoints/model\n",
      "[[step      620]]     [[train]]     loss: 0.4136453        [[val]]     loss: 0.40798449       \n",
      "saving model to ./checkpoints/model\n",
      "[[step      630]]     [[train]]     loss: 0.41256708       [[val]]     loss: 0.4061706        \n",
      "saving model to ./checkpoints/model\n",
      "[[step      640]]     [[train]]     loss: 0.41434004       [[val]]     loss: 0.40447408       \n",
      "saving model to ./checkpoints/model\n",
      "[[step      650]]     [[train]]     loss: 0.41205753       [[val]]     loss: 0.40452244       \n",
      "[[step      660]]     [[train]]     loss: 0.40959781       [[val]]     loss: 0.40291304       \n",
      "saving model to ./checkpoints/model\n",
      "[[step      670]]     [[train]]     loss: 0.41142476       [[val]]     loss: 0.40312303       \n",
      "[[step      680]]     [[train]]     loss: 0.41174147       [[val]]     loss: 0.40336156       \n",
      "[[step      690]]     [[train]]     loss: 0.4141909        [[val]]     loss: 0.40449342       \n",
      "[[step      700]]     [[train]]     loss: 0.41539564       [[val]]     loss: 0.4068011        \n",
      "[[step      710]]     [[train]]     loss: 0.41443741       [[val]]     loss: 0.40744363       \n",
      "[[step      720]]     [[train]]     loss: 0.41476351       [[val]]     loss: 0.4104463        \n",
      "[[step      730]]     [[train]]     loss: 0.41488425       [[val]]     loss: 0.41166261       \n",
      "[[step      740]]     [[train]]     loss: 0.41142246       [[val]]     loss: 0.40977299       \n",
      "[[step      750]]     [[train]]     loss: 0.41143473       [[val]]     loss: 0.40878795       \n",
      "[[step      760]]     [[train]]     loss: 0.41217977       [[val]]     loss: 0.40843338       \n",
      "[[step      770]]     [[train]]     loss: 0.4122065        [[val]]     loss: 0.408766         \n",
      "[[step      780]]     [[train]]     loss: 0.41292092       [[val]]     loss: 0.41012092       \n",
      "[[step      790]]     [[train]]     loss: 0.41097594       [[val]]     loss: 0.40937473       \n",
      "[[step      800]]     [[train]]     loss: 0.40949866       [[val]]     loss: 0.40689187       \n",
      "[[step      810]]     [[train]]     loss: 0.40847791       [[val]]     loss: 0.40666071       \n",
      "[[step      820]]     [[train]]     loss: 0.40739604       [[val]]     loss: 0.40544332       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[step      830]]     [[train]]     loss: 0.4064962        [[val]]     loss: 0.4040835        \n",
      "[[step      840]]     [[train]]     loss: 0.40743832       [[val]]     loss: 0.40501509       \n",
      "[[step      850]]     [[train]]     loss: 0.40868709       [[val]]     loss: 0.40621253       \n",
      "[[step      860]]     [[train]]     loss: 0.40838579       [[val]]     loss: 0.40771009       \n",
      "[[step      870]]     [[train]]     loss: 0.40761503       [[val]]     loss: 0.40739328       \n",
      "[[step      880]]     [[train]]     loss: 0.40634289       [[val]]     loss: 0.40628097       \n",
      "[[step      890]]     [[train]]     loss: 0.40440963       [[val]]     loss: 0.40481689       \n",
      "[[step      900]]     [[train]]     loss: 0.40245982       [[val]]     loss: 0.403078         \n",
      "[[step      910]]     [[train]]     loss: 0.40058285       [[val]]     loss: 0.4036806        \n",
      "[[step      920]]     [[train]]     loss: 0.39944237       [[val]]     loss: 0.40289177       \n",
      "saving model to ./checkpoints/model\n",
      "[[step      930]]     [[train]]     loss: 0.39982045       [[val]]     loss: 0.40364731       \n",
      "[[step      940]]     [[train]]     loss: 0.39967163       [[val]]     loss: 0.40371505       \n",
      "[[step      950]]     [[train]]     loss: 0.39806845       [[val]]     loss: 0.4018906        \n",
      "saving model to ./checkpoints/model\n",
      "[[step      960]]     [[train]]     loss: 0.39785445       [[val]]     loss: 0.39946696       \n",
      "saving model to ./checkpoints/model\n",
      "[[step      970]]     [[train]]     loss: 0.39672689       [[val]]     loss: 0.39711477       \n",
      "saving model to ./checkpoints/model\n",
      "[[step      980]]     [[train]]     loss: 0.39676972       [[val]]     loss: 0.39755292       \n",
      "[[step      990]]     [[train]]     loss: 0.39725096       [[val]]     loss: 0.39714554       \n",
      "[[step     1000]]     [[train]]     loss: 0.39807704       [[val]]     loss: 0.39742989       \n",
      "[[step     1010]]     [[train]]     loss: 0.39877872       [[val]]     loss: 0.3974013        \n",
      "[[step     1020]]     [[train]]     loss: 0.40023233       [[val]]     loss: 0.39697318       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     1030]]     [[train]]     loss: 0.40057385       [[val]]     loss: 0.39639796       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     1040]]     [[train]]     loss: 0.4017797        [[val]]     loss: 0.39566593       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     1050]]     [[train]]     loss: 0.40301089       [[val]]     loss: 0.39654571       \n",
      "[[step     1060]]     [[train]]     loss: 0.4031515        [[val]]     loss: 0.39800682       \n",
      "[[step     1070]]     [[train]]     loss: 0.40491372       [[val]]     loss: 0.40068849       \n",
      "[[step     1080]]     [[train]]     loss: 0.40572445       [[val]]     loss: 0.39946104       \n",
      "[[step     1090]]     [[train]]     loss: 0.40598381       [[val]]     loss: 0.39962861       \n",
      "[[step     1100]]     [[train]]     loss: 0.40701229       [[val]]     loss: 0.39971943       \n",
      "[[step     1110]]     [[train]]     loss: 0.40856519       [[val]]     loss: 0.3982095        \n",
      "[[step     1120]]     [[train]]     loss: 0.40540643       [[val]]     loss: 0.39881915       \n",
      "[[step     1130]]     [[train]]     loss: 0.40411597       [[val]]     loss: 0.39961906       \n",
      "[[step     1140]]     [[train]]     loss: 0.40274015       [[val]]     loss: 0.40118639       \n",
      "[[step     1150]]     [[train]]     loss: 0.40403215       [[val]]     loss: 0.39974425       \n",
      "[[step     1160]]     [[train]]     loss: 0.40350963       [[val]]     loss: 0.39908127       \n",
      "[[step     1170]]     [[train]]     loss: 0.40357424       [[val]]     loss: 0.39772594       \n",
      "[[step     1180]]     [[train]]     loss: 0.40174942       [[val]]     loss: 0.39846313       \n",
      "[[step     1190]]     [[train]]     loss: 0.40054138       [[val]]     loss: 0.39733819       \n",
      "[[step     1200]]     [[train]]     loss: 0.39992867       [[val]]     loss: 0.39797665       \n",
      "[[step     1210]]     [[train]]     loss: 0.40107844       [[val]]     loss: 0.39773588       \n",
      "[[step     1220]]     [[train]]     loss: 0.40325891       [[val]]     loss: 0.39788943       \n",
      "[[step     1230]]     [[train]]     loss: 0.40485728       [[val]]     loss: 0.39804037       \n",
      "[[step     1240]]     [[train]]     loss: 0.40564039       [[val]]     loss: 0.39760696       \n",
      "[[step     1250]]     [[train]]     loss: 0.40447078       [[val]]     loss: 0.39853366       \n",
      "[[step     1260]]     [[train]]     loss: 0.40502563       [[val]]     loss: 0.39883302       \n",
      "[[step     1270]]     [[train]]     loss: 0.40339614       [[val]]     loss: 0.39887017       \n",
      "[[step     1280]]     [[train]]     loss: 0.40303473       [[val]]     loss: 0.39806571       \n",
      "[[step     1290]]     [[train]]     loss: 0.40264714       [[val]]     loss: 0.39836651       \n",
      "[[step     1300]]     [[train]]     loss: 0.40221354       [[val]]     loss: 0.39759926       \n",
      "[[step     1310]]     [[train]]     loss: 0.3993491        [[val]]     loss: 0.39746484       \n",
      "[[step     1320]]     [[train]]     loss: 0.39836444       [[val]]     loss: 0.39645284       \n",
      "[[step     1330]]     [[train]]     loss: 0.39703427       [[val]]     loss: 0.39414469       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     1340]]     [[train]]     loss: 0.39563685       [[val]]     loss: 0.39366096       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     1350]]     [[train]]     loss: 0.39512192       [[val]]     loss: 0.39322961       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     1360]]     [[train]]     loss: 0.39391852       [[val]]     loss: 0.39363833       \n",
      "[[step     1370]]     [[train]]     loss: 0.39520387       [[val]]     loss: 0.39405434       \n",
      "[[step     1380]]     [[train]]     loss: 0.39589205       [[val]]     loss: 0.39382423       \n",
      "[[step     1390]]     [[train]]     loss: 0.39491071       [[val]]     loss: 0.39354658       \n",
      "[[step     1400]]     [[train]]     loss: 0.39491912       [[val]]     loss: 0.39476548       \n",
      "[[step     1410]]     [[train]]     loss: 0.3942779        [[val]]     loss: 0.3959139        \n",
      "[[step     1420]]     [[train]]     loss: 0.39491989       [[val]]     loss: 0.39570032       \n",
      "[[step     1430]]     [[train]]     loss: 0.39463002       [[val]]     loss: 0.3955172        \n",
      "[[step     1440]]     [[train]]     loss: 0.39581045       [[val]]     loss: 0.39579787       \n",
      "[[step     1450]]     [[train]]     loss: 0.39505438       [[val]]     loss: 0.39677617       \n",
      "[[step     1460]]     [[train]]     loss: 0.39532439       [[val]]     loss: 0.39511479       \n",
      "[[step     1470]]     [[train]]     loss: 0.39524255       [[val]]     loss: 0.39503374       \n",
      "[[step     1480]]     [[train]]     loss: 0.39610161       [[val]]     loss: 0.39562363       \n",
      "[[step     1490]]     [[train]]     loss: 0.39928746       [[val]]     loss: 0.39711249       \n",
      "[[step     1500]]     [[train]]     loss: 0.39901009       [[val]]     loss: 0.3945572        \n",
      "[[step     1510]]     [[train]]     loss: 0.40039707       [[val]]     loss: 0.39386362       \n",
      "[[step     1520]]     [[train]]     loss: 0.39853093       [[val]]     loss: 0.3943791        \n",
      "[[step     1530]]     [[train]]     loss: 0.3981283        [[val]]     loss: 0.39387802       \n",
      "[[step     1540]]     [[train]]     loss: 0.39729217       [[val]]     loss: 0.393763         \n",
      "[[step     1550]]     [[train]]     loss: 0.39592842       [[val]]     loss: 0.394429         \n",
      "[[step     1560]]     [[train]]     loss: 0.39549613       [[val]]     loss: 0.39457988       \n",
      "[[step     1570]]     [[train]]     loss: 0.39627005       [[val]]     loss: 0.39521977       \n",
      "[[step     1580]]     [[train]]     loss: 0.39635184       [[val]]     loss: 0.39539635       \n",
      "[[step     1590]]     [[train]]     loss: 0.39462555       [[val]]     loss: 0.39443154       \n",
      "[[step     1600]]     [[train]]     loss: 0.39377493       [[val]]     loss: 0.39598318       \n",
      "[[step     1610]]     [[train]]     loss: 0.39367782       [[val]]     loss: 0.39693973       \n",
      "[[step     1620]]     [[train]]     loss: 0.39484749       [[val]]     loss: 0.39453631       \n",
      "[[step     1630]]     [[train]]     loss: 0.39626448       [[val]]     loss: 0.39681344       \n",
      "[[step     1640]]     [[train]]     loss: 0.39732326       [[val]]     loss: 0.39715674       \n",
      "[[step     1650]]     [[train]]     loss: 0.39969898       [[val]]     loss: 0.39609735       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[step     1660]]     [[train]]     loss: 0.4002198        [[val]]     loss: 0.39756421       \n",
      "[[step     1670]]     [[train]]     loss: 0.39815056       [[val]]     loss: 0.39627847       \n",
      "[[step     1680]]     [[train]]     loss: 0.39883495       [[val]]     loss: 0.3947013        \n",
      "[[step     1690]]     [[train]]     loss: 0.39818022       [[val]]     loss: 0.39354406       \n",
      "[[step     1700]]     [[train]]     loss: 0.39831749       [[val]]     loss: 0.39398231       \n",
      "[[step     1710]]     [[train]]     loss: 0.39875977       [[val]]     loss: 0.39215031       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     1720]]     [[train]]     loss: 0.39871244       [[val]]     loss: 0.39471698       \n",
      "[[step     1730]]     [[train]]     loss: 0.39532427       [[val]]     loss: 0.39391323       \n",
      "[[step     1740]]     [[train]]     loss: 0.39519119       [[val]]     loss: 0.39206645       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     1750]]     [[train]]     loss: 0.39595625       [[val]]     loss: 0.39117634       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     1760]]     [[train]]     loss: 0.39552081       [[val]]     loss: 0.39090078       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     1770]]     [[train]]     loss: 0.3954126        [[val]]     loss: 0.39197335       \n",
      "[[step     1780]]     [[train]]     loss: 0.39359666       [[val]]     loss: 0.39114595       \n",
      "[[step     1790]]     [[train]]     loss: 0.39614476       [[val]]     loss: 0.39248461       \n",
      "[[step     1800]]     [[train]]     loss: 0.39654324       [[val]]     loss: 0.39124825       \n",
      "[[step     1810]]     [[train]]     loss: 0.39585087       [[val]]     loss: 0.39166282       \n",
      "[[step     1820]]     [[train]]     loss: 0.39639181       [[val]]     loss: 0.39141062       \n",
      "[[step     1830]]     [[train]]     loss: 0.39839492       [[val]]     loss: 0.39166525       \n",
      "[[step     1840]]     [[train]]     loss: 0.39559692       [[val]]     loss: 0.39288644       \n",
      "[[step     1850]]     [[train]]     loss: 0.39346155       [[val]]     loss: 0.39309262       \n",
      "[[step     1860]]     [[train]]     loss: 0.3933266        [[val]]     loss: 0.39369166       \n",
      "[[step     1870]]     [[train]]     loss: 0.39330767       [[val]]     loss: 0.39184433       \n",
      "[[step     1880]]     [[train]]     loss: 0.39185961       [[val]]     loss: 0.39263403       \n",
      "[[step     1890]]     [[train]]     loss: 0.39056144       [[val]]     loss: 0.39170425       \n",
      "[[step     1900]]     [[train]]     loss: 0.39123286       [[val]]     loss: 0.39254431       \n",
      "[[step     1910]]     [[train]]     loss: 0.39263799       [[val]]     loss: 0.39360315       \n",
      "[[step     1920]]     [[train]]     loss: 0.39195459       [[val]]     loss: 0.39350336       \n",
      "[[step     1930]]     [[train]]     loss: 0.39218411       [[val]]     loss: 0.39331549       \n",
      "[[step     1940]]     [[train]]     loss: 0.39449388       [[val]]     loss: 0.39246523       \n",
      "[[step     1950]]     [[train]]     loss: 0.39472113       [[val]]     loss: 0.39218459       \n",
      "[[step     1960]]     [[train]]     loss: 0.39365264       [[val]]     loss: 0.39021507       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     1970]]     [[train]]     loss: 0.39292396       [[val]]     loss: 0.39277656       \n",
      "[[step     1980]]     [[train]]     loss: 0.39381555       [[val]]     loss: 0.39288805       \n",
      "[[step     1990]]     [[train]]     loss: 0.39368969       [[val]]     loss: 0.39285588       \n",
      "[[step     2000]]     [[train]]     loss: 0.39279655       [[val]]     loss: 0.39343338       \n",
      "[[step     2010]]     [[train]]     loss: 0.3924003        [[val]]     loss: 0.39083819       \n",
      "[[step     2020]]     [[train]]     loss: 0.39274935       [[val]]     loss: 0.38992052       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     2030]]     [[train]]     loss: 0.39170575       [[val]]     loss: 0.38893414       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     2040]]     [[train]]     loss: 0.39158787       [[val]]     loss: 0.38819724       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     2050]]     [[train]]     loss: 0.39139258       [[val]]     loss: 0.38813233       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     2060]]     [[train]]     loss: 0.39288355       [[val]]     loss: 0.38804947       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     2070]]     [[train]]     loss: 0.39467555       [[val]]     loss: 0.38857384       \n",
      "[[step     2080]]     [[train]]     loss: 0.39545795       [[val]]     loss: 0.38882948       \n",
      "[[step     2090]]     [[train]]     loss: 0.39537002       [[val]]     loss: 0.38895581       \n",
      "[[step     2100]]     [[train]]     loss: 0.39514663       [[val]]     loss: 0.38726944       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     2110]]     [[train]]     loss: 0.39366942       [[val]]     loss: 0.38910402       \n",
      "[[step     2120]]     [[train]]     loss: 0.39365819       [[val]]     loss: 0.38989618       \n",
      "[[step     2130]]     [[train]]     loss: 0.39459613       [[val]]     loss: 0.38971911       \n",
      "[[step     2140]]     [[train]]     loss: 0.39344305       [[val]]     loss: 0.39083826       \n",
      "[[step     2150]]     [[train]]     loss: 0.39165055       [[val]]     loss: 0.39120029       \n",
      "[[step     2160]]     [[train]]     loss: 0.39091403       [[val]]     loss: 0.39229248       \n",
      "[[step     2170]]     [[train]]     loss: 0.3907804        [[val]]     loss: 0.39019968       \n",
      "[[step     2180]]     [[train]]     loss: 0.39007251       [[val]]     loss: 0.38948325       \n",
      "[[step     2190]]     [[train]]     loss: 0.38891525       [[val]]     loss: 0.38988361       \n",
      "[[step     2200]]     [[train]]     loss: 0.38886323       [[val]]     loss: 0.39045516       \n",
      "[[step     2210]]     [[train]]     loss: 0.38838069       [[val]]     loss: 0.38835466       \n",
      "[[step     2220]]     [[train]]     loss: 0.38907773       [[val]]     loss: 0.38740518       \n",
      "[[step     2230]]     [[train]]     loss: 0.38930333       [[val]]     loss: 0.38781833       \n",
      "[[step     2240]]     [[train]]     loss: 0.3895737        [[val]]     loss: 0.38852535       \n",
      "[[step     2250]]     [[train]]     loss: 0.39186342       [[val]]     loss: 0.38780566       \n",
      "[[step     2260]]     [[train]]     loss: 0.39098666       [[val]]     loss: 0.38601923       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     2270]]     [[train]]     loss: 0.39060115       [[val]]     loss: 0.38632505       \n",
      "[[step     2280]]     [[train]]     loss: 0.39025255       [[val]]     loss: 0.38669273       \n",
      "[[step     2290]]     [[train]]     loss: 0.39106742       [[val]]     loss: 0.38651517       \n",
      "[[step     2300]]     [[train]]     loss: 0.39153201       [[val]]     loss: 0.3867182        \n",
      "[[step     2310]]     [[train]]     loss: 0.3922076        [[val]]     loss: 0.38806924       \n",
      "[[step     2320]]     [[train]]     loss: 0.39117796       [[val]]     loss: 0.38855151       \n",
      "[[step     2330]]     [[train]]     loss: 0.38986129       [[val]]     loss: 0.39019003       \n",
      "[[step     2340]]     [[train]]     loss: 0.39035228       [[val]]     loss: 0.38917445       \n",
      "[[step     2350]]     [[train]]     loss: 0.39035835       [[val]]     loss: 0.38909039       \n",
      "[[step     2360]]     [[train]]     loss: 0.39155822       [[val]]     loss: 0.39108196       \n",
      "[[step     2370]]     [[train]]     loss: 0.39159109       [[val]]     loss: 0.39040964       \n",
      "[[step     2380]]     [[train]]     loss: 0.39177363       [[val]]     loss: 0.39090501       \n",
      "[[step     2390]]     [[train]]     loss: 0.39125327       [[val]]     loss: 0.38959822       \n",
      "[[step     2400]]     [[train]]     loss: 0.39028748       [[val]]     loss: 0.38957327       \n",
      "[[step     2410]]     [[train]]     loss: 0.38955979       [[val]]     loss: 0.39012627       \n",
      "[[step     2420]]     [[train]]     loss: 0.38876113       [[val]]     loss: 0.39030998       \n",
      "[[step     2430]]     [[train]]     loss: 0.3901261        [[val]]     loss: 0.38912377       \n",
      "[[step     2440]]     [[train]]     loss: 0.39100445       [[val]]     loss: 0.38912468       \n",
      "[[step     2450]]     [[train]]     loss: 0.3902529        [[val]]     loss: 0.39039263       \n",
      "[[step     2460]]     [[train]]     loss: 0.38966176       [[val]]     loss: 0.38834472       \n",
      "[[step     2470]]     [[train]]     loss: 0.38803625       [[val]]     loss: 0.38855481       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[step     2480]]     [[train]]     loss: 0.38860628       [[val]]     loss: 0.38809025       \n",
      "[[step     2490]]     [[train]]     loss: 0.38880711       [[val]]     loss: 0.38961738       \n",
      "[[step     2500]]     [[train]]     loss: 0.38956885       [[val]]     loss: 0.3891672        \n",
      "[[step     2510]]     [[train]]     loss: 0.3899986        [[val]]     loss: 0.38743731       \n",
      "[[step     2520]]     [[train]]     loss: 0.39131638       [[val]]     loss: 0.3875954        \n",
      "[[step     2530]]     [[train]]     loss: 0.39096648       [[val]]     loss: 0.38750104       \n",
      "[[step     2540]]     [[train]]     loss: 0.38986171       [[val]]     loss: 0.3883504        \n",
      "[[step     2550]]     [[train]]     loss: 0.39143296       [[val]]     loss: 0.38805995       \n",
      "[[step     2560]]     [[train]]     loss: 0.39275913       [[val]]     loss: 0.39061999       \n",
      "[[step     2570]]     [[train]]     loss: 0.39388446       [[val]]     loss: 0.39098407       \n",
      "[[step     2580]]     [[train]]     loss: 0.3939533        [[val]]     loss: 0.3900362        \n",
      "[[step     2590]]     [[train]]     loss: 0.39498466       [[val]]     loss: 0.38945026       \n",
      "[[step     2600]]     [[train]]     loss: 0.39558661       [[val]]     loss: 0.39010623       \n",
      "[[step     2610]]     [[train]]     loss: 0.39611202       [[val]]     loss: 0.39090453       \n",
      "[[step     2620]]     [[train]]     loss: 0.39465745       [[val]]     loss: 0.39094775       \n",
      "[[step     2630]]     [[train]]     loss: 0.39477793       [[val]]     loss: 0.39115633       \n",
      "[[step     2640]]     [[train]]     loss: 0.39313531       [[val]]     loss: 0.39008544       \n",
      "[[step     2650]]     [[train]]     loss: 0.39255184       [[val]]     loss: 0.38977894       \n",
      "[[step     2660]]     [[train]]     loss: 0.39142877       [[val]]     loss: 0.38769446       \n",
      "[[step     2670]]     [[train]]     loss: 0.39096772       [[val]]     loss: 0.38679807       \n",
      "[[step     2680]]     [[train]]     loss: 0.39007093       [[val]]     loss: 0.38905518       \n",
      "[[step     2690]]     [[train]]     loss: 0.38881778       [[val]]     loss: 0.38958761       \n",
      "[[step     2700]]     [[train]]     loss: 0.38842372       [[val]]     loss: 0.38882195       \n",
      "[[step     2710]]     [[train]]     loss: 0.38957123       [[val]]     loss: 0.38959696       \n",
      "[[step     2720]]     [[train]]     loss: 0.39031659       [[val]]     loss: 0.38859759       \n",
      "[[step     2730]]     [[train]]     loss: 0.38930036       [[val]]     loss: 0.38820387       \n",
      "[[step     2740]]     [[train]]     loss: 0.39095164       [[val]]     loss: 0.38767327       \n",
      "[[step     2750]]     [[train]]     loss: 0.39018534       [[val]]     loss: 0.38666766       \n",
      "[[step     2760]]     [[train]]     loss: 0.3913027        [[val]]     loss: 0.38754456       \n",
      "[[step     2770]]     [[train]]     loss: 0.39142775       [[val]]     loss: 0.38724257       \n",
      "[[step     2780]]     [[train]]     loss: 0.39253796       [[val]]     loss: 0.38550733       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     2790]]     [[train]]     loss: 0.39182946       [[val]]     loss: 0.38530599       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     2800]]     [[train]]     loss: 0.39188116       [[val]]     loss: 0.38507871       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     2810]]     [[train]]     loss: 0.39088011       [[val]]     loss: 0.38505264       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     2820]]     [[train]]     loss: 0.39113327       [[val]]     loss: 0.38609839       \n",
      "[[step     2830]]     [[train]]     loss: 0.39142          [[val]]     loss: 0.38506098       \n",
      "[[step     2840]]     [[train]]     loss: 0.39118976       [[val]]     loss: 0.38534349       \n",
      "[[step     2850]]     [[train]]     loss: 0.39066909       [[val]]     loss: 0.38554473       \n",
      "[[step     2860]]     [[train]]     loss: 0.38865592       [[val]]     loss: 0.38394431       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     2870]]     [[train]]     loss: 0.38920122       [[val]]     loss: 0.38471343       \n",
      "[[step     2880]]     [[train]]     loss: 0.3870976        [[val]]     loss: 0.38542542       \n",
      "[[step     2890]]     [[train]]     loss: 0.38813517       [[val]]     loss: 0.38527338       \n",
      "[[step     2900]]     [[train]]     loss: 0.38672324       [[val]]     loss: 0.38568415       \n",
      "[[step     2910]]     [[train]]     loss: 0.38699801       [[val]]     loss: 0.38564786       \n",
      "[[step     2920]]     [[train]]     loss: 0.38661526       [[val]]     loss: 0.3849148        \n",
      "[[step     2930]]     [[train]]     loss: 0.38698395       [[val]]     loss: 0.38606575       \n",
      "[[step     2940]]     [[train]]     loss: 0.38792055       [[val]]     loss: 0.38529384       \n",
      "[[step     2950]]     [[train]]     loss: 0.38848958       [[val]]     loss: 0.38534315       \n",
      "[[step     2960]]     [[train]]     loss: 0.38903579       [[val]]     loss: 0.38726922       \n",
      "[[step     2970]]     [[train]]     loss: 0.38856661       [[val]]     loss: 0.38661413       \n",
      "[[step     2980]]     [[train]]     loss: 0.3902544        [[val]]     loss: 0.38617064       \n",
      "[[step     2990]]     [[train]]     loss: 0.38914535       [[val]]     loss: 0.38568335       \n",
      "[[step     3000]]     [[train]]     loss: 0.38941432       [[val]]     loss: 0.38576665       \n",
      "[[step     3010]]     [[train]]     loss: 0.38806787       [[val]]     loss: 0.38445301       \n",
      "[[step     3020]]     [[train]]     loss: 0.3882524        [[val]]     loss: 0.38405296       \n",
      "[[step     3030]]     [[train]]     loss: 0.38764954       [[val]]     loss: 0.3830841        \n",
      "saving model to ./checkpoints/model\n",
      "[[step     3040]]     [[train]]     loss: 0.38715192       [[val]]     loss: 0.38459426       \n",
      "[[step     3050]]     [[train]]     loss: 0.38567255       [[val]]     loss: 0.38354357       \n",
      "[[step     3060]]     [[train]]     loss: 0.38525124       [[val]]     loss: 0.38304624       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     3070]]     [[train]]     loss: 0.38510789       [[val]]     loss: 0.38232552       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     3080]]     [[train]]     loss: 0.3865908        [[val]]     loss: 0.38352605       \n",
      "[[step     3090]]     [[train]]     loss: 0.38817943       [[val]]     loss: 0.38479822       \n",
      "[[step     3100]]     [[train]]     loss: 0.38977929       [[val]]     loss: 0.38532587       \n",
      "[[step     3110]]     [[train]]     loss: 0.39099508       [[val]]     loss: 0.38697475       \n",
      "[[step     3120]]     [[train]]     loss: 0.39040978       [[val]]     loss: 0.38594296       \n",
      "[[step     3130]]     [[train]]     loss: 0.39043361       [[val]]     loss: 0.38674746       \n",
      "[[step     3140]]     [[train]]     loss: 0.3899124        [[val]]     loss: 0.38516886       \n",
      "[[step     3150]]     [[train]]     loss: 0.39075173       [[val]]     loss: 0.3864759        \n",
      "[[step     3160]]     [[train]]     loss: 0.39101693       [[val]]     loss: 0.38595292       \n",
      "[[step     3170]]     [[train]]     loss: 0.39091563       [[val]]     loss: 0.38649711       \n",
      "[[step     3180]]     [[train]]     loss: 0.3898108        [[val]]     loss: 0.38503082       \n",
      "[[step     3190]]     [[train]]     loss: 0.39024682       [[val]]     loss: 0.38472257       \n",
      "[[step     3200]]     [[train]]     loss: 0.38957439       [[val]]     loss: 0.3834202        \n",
      "[[step     3210]]     [[train]]     loss: 0.38778297       [[val]]     loss: 0.38237796       \n",
      "[[step     3220]]     [[train]]     loss: 0.38819475       [[val]]     loss: 0.38268197       \n",
      "[[step     3230]]     [[train]]     loss: 0.38790828       [[val]]     loss: 0.38225385       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     3240]]     [[train]]     loss: 0.38807073       [[val]]     loss: 0.38282196       \n",
      "[[step     3250]]     [[train]]     loss: 0.38894311       [[val]]     loss: 0.38212591       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     3260]]     [[train]]     loss: 0.38865624       [[val]]     loss: 0.38223251       \n",
      "[[step     3270]]     [[train]]     loss: 0.38777234       [[val]]     loss: 0.38168838       \n",
      "saving model to ./checkpoints/model\n",
      "[[step     3280]]     [[train]]     loss: 0.38670303       [[val]]     loss: 0.38239135       \n",
      "[[step     3290]]     [[train]]     loss: 0.38550755       [[val]]     loss: 0.38133642       \n",
      "saving model to ./checkpoints/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[step     3300]]     [[train]]     loss: 0.38441389       [[val]]     loss: 0.38280504       \n",
      "[[step     3310]]     [[train]]     loss: 0.38662683       [[val]]     loss: 0.38409613       \n",
      "[[step     3320]]     [[train]]     loss: 0.38662361       [[val]]     loss: 0.38481881       \n",
      "[[step     3330]]     [[train]]     loss: 0.38699614       [[val]]     loss: 0.38542697       \n",
      "[[step     3340]]     [[train]]     loss: 0.38778566       [[val]]     loss: 0.38580315       \n",
      "[[step     3350]]     [[train]]     loss: 0.38820301       [[val]]     loss: 0.3872174        \n",
      "[[step     3360]]     [[train]]     loss: 0.38989293       [[val]]     loss: 0.38757841       \n",
      "[[step     3370]]     [[train]]     loss: 0.39095486       [[val]]     loss: 0.38970816       \n",
      "[[step     3380]]     [[train]]     loss: 0.39280639       [[val]]     loss: 0.38927865       \n",
      "[[step     3390]]     [[train]]     loss: 0.39269295       [[val]]     loss: 0.38959955       \n",
      "[[step     3400]]     [[train]]     loss: 0.39433346       [[val]]     loss: 0.38815957       \n",
      "[[step     3410]]     [[train]]     loss: 0.39343623       [[val]]     loss: 0.38911891       \n",
      "[[step     3420]]     [[train]]     loss: 0.39256624       [[val]]     loss: 0.38931732       \n",
      "[[step     3430]]     [[train]]     loss: 0.39246396       [[val]]     loss: 0.38853503       \n",
      "[[step     3440]]     [[train]]     loss: 0.39134513       [[val]]     loss: 0.38924156       \n",
      "[[step     3450]]     [[train]]     loss: 0.38965234       [[val]]     loss: 0.38794854       \n",
      "[[step     3460]]     [[train]]     loss: 0.38995854       [[val]]     loss: 0.38772582       \n",
      "[[step     3470]]     [[train]]     loss: 0.38915724       [[val]]     loss: 0.38700013       \n",
      "[[step     3480]]     [[train]]     loss: 0.386697         [[val]]     loss: 0.38689295       \n",
      "[[step     3490]]     [[train]]     loss: 0.38677214       [[val]]     loss: 0.38663715       \n",
      "[[step     3500]]     [[train]]     loss: 0.38655964       [[val]]     loss: 0.38711015       \n",
      "[[step     3510]]     [[train]]     loss: 0.38603882       [[val]]     loss: 0.38493315       \n",
      "[[step     3520]]     [[train]]     loss: 0.38744809       [[val]]     loss: 0.38464844       \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_frame import DataFrame\n",
    "from tf_base_model import TFBaseModel\n",
    "from tf_utils import (\n",
    "    time_distributed_dense_layer, temporal_convolution_layer,\n",
    "    sequence_mean, sequence_smape, shape\n",
    ")\n",
    "\n",
    "\n",
    "class DataReader(object):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'data',\n",
    "            'is_nan',\n",
    "            'page_id',\n",
    "            'project',\n",
    "            'access',\n",
    "            'agent',\n",
    "            'test_data',\n",
    "            'test_is_nan'\n",
    "        ]\n",
    "        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i))) for i in data_cols]\n",
    "\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data)\n",
    "        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.95)\n",
    "\n",
    "        print ('train size', len(self.train_df))\n",
    "        print ('val size', len(self.val_df))\n",
    "        print ('test size', len(self.test_df))\n",
    "\n",
    "    def train_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "\n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        batch_gen = df.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=num_epochs,\n",
    "            allow_smaller_final_batch=is_test\n",
    "        )\n",
    "        data_col = 'test_data' if is_test else 'data'\n",
    "        is_nan_col = 'test_is_nan' if is_test else 'is_nan'\n",
    "        for batch in batch_gen:\n",
    "            num_decode_steps = 64\n",
    "            full_seq_len = batch[data_col].shape[1]\n",
    "            max_encode_length = full_seq_len - num_decode_steps if not is_test else full_seq_len\n",
    "\n",
    "            x_encode = np.zeros([len(batch), max_encode_length])\n",
    "            y_decode = np.zeros([len(batch), num_decode_steps])\n",
    "            is_nan_encode = np.zeros([len(batch), max_encode_length])\n",
    "            is_nan_decode = np.zeros([len(batch), num_decode_steps])\n",
    "            encode_len = np.zeros([len(batch)])\n",
    "            decode_len = np.zeros([len(batch)])\n",
    "\n",
    "            for i, (seq, nan_seq) in enumerate(zip(batch[data_col], batch[is_nan_col])):\n",
    "                rand_len = np.random.randint(max_encode_length - 365 + 1, max_encode_length + 1)\n",
    "                x_encode_len = max_encode_length if is_test else rand_len\n",
    "                x_encode[i, :x_encode_len] = seq[:x_encode_len]\n",
    "                is_nan_encode[i, :x_encode_len] = nan_seq[:x_encode_len]\n",
    "                encode_len[i] = x_encode_len\n",
    "                decode_len[i] = num_decode_steps\n",
    "                if not is_test:\n",
    "                    y_decode[i, :] = seq[x_encode_len: x_encode_len + num_decode_steps]\n",
    "                    is_nan_decode[i, :] = nan_seq[x_encode_len: x_encode_len + num_decode_steps]\n",
    "\n",
    "            batch['x_encode'] = x_encode\n",
    "            batch['encode_len'] = encode_len\n",
    "            batch['y_decode'] = y_decode\n",
    "            batch['decode_len'] = decode_len\n",
    "            batch['is_nan_encode'] = is_nan_encode\n",
    "            batch['is_nan_decode'] = is_nan_decode\n",
    "\n",
    "            yield batch\n",
    "\n",
    "\n",
    "class cnn(TFBaseModel):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        residual_channels=32,\n",
    "        skip_channels=32,\n",
    "        dilations=[2**i for i in range(8)]*3,\n",
    "        filter_widths=[2 for i in range(8)]*3,\n",
    "        num_decode_steps=64,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.dilations = dilations\n",
    "        self.filter_widths = filter_widths\n",
    "        self.num_decode_steps = num_decode_steps\n",
    "        super(cnn, self).__init__(**kwargs)\n",
    "\n",
    "    def transform(self, x):\n",
    "        return tf.log(x + 1) - tf.expand_dims(self.log_x_encode_mean, 1)\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return tf.exp(x + tf.expand_dims(self.log_x_encode_mean, 1)) - 1\n",
    "\n",
    "    def get_input_sequences(self):\n",
    "        self.x_encode = tf.placeholder(tf.float32, [None, None])\n",
    "        self.encode_len = tf.placeholder(tf.int32, [None])\n",
    "        self.y_decode = tf.placeholder(tf.float32, [None, self.num_decode_steps])\n",
    "        self.decode_len = tf.placeholder(tf.int32, [None])\n",
    "        self.is_nan_encode = tf.placeholder(tf.float32, [None, None])\n",
    "        self.is_nan_decode = tf.placeholder(tf.float32, [None, self.num_decode_steps])\n",
    "\n",
    "        self.page_id = tf.placeholder(tf.int32, [None])\n",
    "        self.project = tf.placeholder(tf.int32, [None])\n",
    "        self.access = tf.placeholder(tf.int32, [None])\n",
    "        self.agent = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        self.log_x_encode_mean = sequence_mean(tf.log(self.x_encode + 1), self.encode_len)\n",
    "        self.log_x_encode = self.transform(self.x_encode)\n",
    "        self.x = tf.expand_dims(self.log_x_encode, 2)\n",
    "\n",
    "        self.encode_features = tf.concat([\n",
    "            tf.expand_dims(self.is_nan_encode, 2),\n",
    "            tf.expand_dims(tf.cast(tf.equal(self.x_encode, 0.0), tf.float32), 2),\n",
    "            tf.tile(tf.reshape(self.log_x_encode_mean, (-1, 1, 1)), (1, tf.shape(self.x_encode)[1], 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.project, 9), 1), (1, tf.shape(self.x_encode)[1], 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.access, 3), 1), (1, tf.shape(self.x_encode)[1], 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.agent, 2), 1), (1, tf.shape(self.x_encode)[1], 1)),\n",
    "        ], axis=2)\n",
    "\n",
    "        decode_idx = tf.tile(tf.expand_dims(tf.range(self.num_decode_steps), 0), (tf.shape(self.y_decode)[0], 1))\n",
    "        self.decode_features = tf.concat([\n",
    "            tf.one_hot(decode_idx, self.num_decode_steps),\n",
    "            tf.tile(tf.reshape(self.log_x_encode_mean, (-1, 1, 1)), (1, self.num_decode_steps, 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.project, 9), 1), (1, self.num_decode_steps, 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.access, 3), 1), (1, self.num_decode_steps, 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.agent, 2), 1), (1, self.num_decode_steps, 1)),\n",
    "        ], axis=2)\n",
    "\n",
    "        return self.x\n",
    "\n",
    "    def encode(self, x, features):\n",
    "        x = tf.concat([x, features], axis=2)\n",
    "\n",
    "        inputs = time_distributed_dense_layer(\n",
    "            inputs=x,\n",
    "            output_units=self.residual_channels,\n",
    "            activation=tf.nn.tanh,\n",
    "            scope='x-proj-encode'\n",
    "        )\n",
    "\n",
    "        skip_outputs = []\n",
    "        conv_inputs = [inputs]\n",
    "        for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n",
    "            dilated_conv = temporal_convolution_layer(\n",
    "                inputs=inputs,\n",
    "                output_units=2*self.residual_channels,\n",
    "                convolution_width=filter_width,\n",
    "                causal=True,\n",
    "                dilation_rate=[dilation],\n",
    "                scope='dilated-conv-encode-{}'.format(i)\n",
    "            )\n",
    "            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "            dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "            outputs = time_distributed_dense_layer(\n",
    "                inputs=dilated_conv,\n",
    "                output_units=self.skip_channels + self.residual_channels,\n",
    "                scope='dilated-conv-proj-encode-{}'.format(i)\n",
    "            )\n",
    "            skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n",
    "\n",
    "            inputs += residuals\n",
    "            conv_inputs.append(inputs)\n",
    "            skip_outputs.append(skips)\n",
    "\n",
    "        skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n",
    "        h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-encode-1', activation=tf.nn.relu)\n",
    "        y_hat = time_distributed_dense_layer(h, 1, scope='dense-encode-2')\n",
    "\n",
    "        return y_hat, conv_inputs[:-1]\n",
    "\n",
    "    def initialize_decode_params(self, x, features):\n",
    "        x = tf.concat([x, features], axis=2)\n",
    "\n",
    "        inputs = time_distributed_dense_layer(\n",
    "            inputs=x,\n",
    "            output_units=self.residual_channels,\n",
    "            activation=tf.nn.tanh,\n",
    "            scope='x-proj-decode'\n",
    "        )\n",
    "\n",
    "        skip_outputs = []\n",
    "        conv_inputs = [inputs]\n",
    "        for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n",
    "            dilated_conv = temporal_convolution_layer(\n",
    "                inputs=inputs,\n",
    "                output_units=2*self.residual_channels,\n",
    "                convolution_width=filter_width,\n",
    "                causal=True,\n",
    "                dilation_rate=[dilation],\n",
    "                scope='dilated-conv-decode-{}'.format(i)\n",
    "            )\n",
    "            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "            dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "            outputs = time_distributed_dense_layer(\n",
    "                inputs=dilated_conv,\n",
    "                output_units=self.skip_channels + self.residual_channels,\n",
    "                scope='dilated-conv-proj-decode-{}'.format(i)\n",
    "            )\n",
    "            skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n",
    "\n",
    "            inputs += residuals\n",
    "            conv_inputs.append(inputs)\n",
    "            skip_outputs.append(skips)\n",
    "\n",
    "        skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n",
    "        h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-decode-1', activation=tf.nn.relu)\n",
    "        y_hat = time_distributed_dense_layer(h, 1, scope='dense-decode-2')\n",
    "        return y_hat\n",
    "\n",
    "    def decode(self, x, conv_inputs, features):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # initialize state tensor arrays\n",
    "        state_queues = []\n",
    "        for i, (conv_input, dilation) in enumerate(zip(conv_inputs, self.dilations)):\n",
    "            batch_idx = tf.range(batch_size)\n",
    "            batch_idx = tf.tile(tf.expand_dims(batch_idx, 1), (1, dilation))\n",
    "            batch_idx = tf.reshape(batch_idx, [-1])\n",
    "\n",
    "            queue_begin_time = self.encode_len - dilation - 1\n",
    "            temporal_idx = tf.expand_dims(queue_begin_time, 1) + tf.expand_dims(tf.range(dilation), 0)\n",
    "            temporal_idx = tf.reshape(temporal_idx, [-1])\n",
    "\n",
    "            idx = tf.stack([batch_idx, temporal_idx], axis=1)\n",
    "            slices = tf.reshape(tf.gather_nd(conv_input, idx), (batch_size, dilation, shape(conv_input, 2)))\n",
    "\n",
    "            layer_ta = tf.TensorArray(dtype=tf.float32, size=dilation + self.num_decode_steps)\n",
    "            layer_ta = layer_ta.unstack(tf.transpose(slices, (1, 0, 2)))\n",
    "            state_queues.append(layer_ta)\n",
    "\n",
    "        # initialize feature tensor array\n",
    "        features_ta = tf.TensorArray(dtype=tf.float32, size=self.num_decode_steps)\n",
    "        features_ta = features_ta.unstack(tf.transpose(features, (1, 0, 2)))\n",
    "\n",
    "        # initialize output tensor array\n",
    "        emit_ta = tf.TensorArray(size=self.num_decode_steps, dtype=tf.float32)\n",
    "\n",
    "        # initialize other loop vars\n",
    "        elements_finished = 0 >= self.decode_len\n",
    "        time = tf.constant(0, dtype=tf.int32)\n",
    "\n",
    "        # get initial x input\n",
    "        current_idx = tf.stack([tf.range(tf.shape(self.encode_len)[0]), self.encode_len - 1], axis=1)\n",
    "        initial_input = tf.gather_nd(x, current_idx)\n",
    "\n",
    "        def loop_fn(time, current_input, queues):\n",
    "            current_features = features_ta.read(time)\n",
    "            current_input = tf.concat([current_input, current_features], axis=1)\n",
    "\n",
    "            with tf.variable_scope('x-proj-decode', reuse=True):\n",
    "                w_x_proj = tf.get_variable('weights')\n",
    "                b_x_proj = tf.get_variable('biases')\n",
    "                x_proj = tf.nn.tanh(tf.matmul(current_input, w_x_proj) + b_x_proj)\n",
    "\n",
    "            skip_outputs, updated_queues = [], []\n",
    "            for i, (conv_input, queue, dilation) in enumerate(zip(conv_inputs, queues, self.dilations)):\n",
    "\n",
    "                state = queue.read(time)\n",
    "                with tf.variable_scope('dilated-conv-decode-{}'.format(i), reuse=True):\n",
    "                    w_conv = tf.get_variable('weights'.format(i))\n",
    "                    b_conv = tf.get_variable('biases'.format(i))\n",
    "                    dilated_conv = tf.matmul(state, w_conv[0, :, :]) + tf.matmul(x_proj, w_conv[1, :, :]) + b_conv\n",
    "                conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=1)\n",
    "                dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "                with tf.variable_scope('dilated-conv-proj-decode-{}'.format(i), reuse=True):\n",
    "                    w_proj = tf.get_variable('weights'.format(i))\n",
    "                    b_proj = tf.get_variable('biases'.format(i))\n",
    "                    concat_outputs = tf.matmul(dilated_conv, w_proj) + b_proj\n",
    "                skips, residuals = tf.split(concat_outputs, [self.skip_channels, self.residual_channels], axis=1)\n",
    "\n",
    "                x_proj += residuals\n",
    "                skip_outputs.append(skips)\n",
    "                updated_queues.append(queue.write(time + dilation, x_proj))\n",
    "\n",
    "            skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=1))\n",
    "            with tf.variable_scope('dense-decode-1', reuse=True):\n",
    "                w_h = tf.get_variable('weights')\n",
    "                b_h = tf.get_variable('biases')\n",
    "                h = tf.nn.relu(tf.matmul(skip_outputs, w_h) + b_h)\n",
    "\n",
    "            with tf.variable_scope('dense-decode-2', reuse=True):\n",
    "                w_y = tf.get_variable('weights')\n",
    "                b_y = tf.get_variable('biases')\n",
    "                y_hat = tf.matmul(h, w_y) + b_y\n",
    "\n",
    "            elements_finished = (time >= self.decode_len)\n",
    "            finished = tf.reduce_all(elements_finished)\n",
    "\n",
    "            next_input = tf.cond(\n",
    "                finished,\n",
    "                lambda: tf.zeros([batch_size, 1], dtype=tf.float32),\n",
    "                lambda: y_hat\n",
    "            )\n",
    "            next_elements_finished = (time >= self.decode_len - 1)\n",
    "\n",
    "            return (next_elements_finished, next_input, updated_queues)\n",
    "\n",
    "        def condition(unused_time, elements_finished, *_):\n",
    "            return tf.logical_not(tf.reduce_all(elements_finished))\n",
    "\n",
    "        def body(time, elements_finished, emit_ta, *state_queues):\n",
    "            (next_finished, emit_output, state_queues) = loop_fn(time, initial_input, state_queues)\n",
    "\n",
    "            emit = tf.where(elements_finished, tf.zeros_like(emit_output), emit_output)\n",
    "            emit_ta = emit_ta.write(time, emit)\n",
    "\n",
    "            elements_finished = tf.logical_or(elements_finished, next_finished)\n",
    "            return [time + 1, elements_finished, emit_ta] + list(state_queues)\n",
    "\n",
    "        returned = tf.while_loop(\n",
    "            cond=condition,\n",
    "            body=body,\n",
    "            loop_vars=[time, elements_finished, emit_ta] + state_queues\n",
    "        )\n",
    "\n",
    "        outputs_ta = returned[2]\n",
    "        y_hat = tf.transpose(outputs_ta.stack(), (1, 0, 2))\n",
    "        return y_hat\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        x = self.get_input_sequences()\n",
    "\n",
    "        y_hat_encode, conv_inputs = self.encode(x, features=self.encode_features)\n",
    "        self.initialize_decode_params(x, features=self.decode_features)\n",
    "        y_hat_decode = self.decode(y_hat_encode, conv_inputs, features=self.decode_features)\n",
    "        y_hat_decode = self.inverse_transform(tf.squeeze(y_hat_decode, 2))\n",
    "        y_hat_decode = tf.nn.relu(y_hat_decode)\n",
    "\n",
    "        self.labels = self.y_decode\n",
    "        self.preds = y_hat_decode\n",
    "        self.loss = sequence_smape(self.labels, self.preds, self.decode_len, self.is_nan_decode)\n",
    "\n",
    "        self.prediction_tensors = {\n",
    "            'priors': self.x_encode,\n",
    "            'labels': self.labels,\n",
    "            'preds': self.preds,\n",
    "            'page_id': self.page_id,\n",
    "        }\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    base_dir = './'\n",
    "\n",
    "    dr = DataReader(data_dir=os.path.join(base_dir, 'data/processed/'))\n",
    "\n",
    "    nn = cnn(\n",
    "        reader=dr,\n",
    "        log_dir=os.path.join(base_dir, 'logs'),\n",
    "        checkpoint_dir=os.path.join(base_dir, 'checkpoints'),\n",
    "        prediction_dir=os.path.join(base_dir, 'predictions'),\n",
    "        optimizer='adam',\n",
    "        learning_rate=.001,\n",
    "        batch_size=128,\n",
    "        num_training_steps=200000,\n",
    "        early_stopping_steps=5000,\n",
    "        warm_start_init_step=0,\n",
    "        regularization_constant=0.0,\n",
    "        keep_prob=1.0,\n",
    "        enable_parameter_averaging=False,\n",
    "        num_restarts=2,\n",
    "        min_steps_to_checkpoint=500,\n",
    "        log_interval=10,\n",
    "        num_validation_batches=1,\n",
    "        grad_clip=20,\n",
    "        residual_channels=32,\n",
    "        skip_channels=32,\n",
    "        dilations=[2**i for i in range(8)]*3,\n",
    "        filter_widths=[2 for i in range(8)]*3,\n",
    "        num_decode_steps=64,\n",
    "    )\n",
    "    nn.fit()\n",
    "    nn.restore()\n",
    "    nn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "lengths_mat = np.load(os.path.join('predictions', 'lengths.npy'))\n",
    "preds_mat = np.load(os.path.join('predictions', 'preds.npy'))\n",
    "ids_mat = np.load(os.path.join('predictions', 'ids.npy'))\n",
    "preds_mat[lengths_mat == 0] = 0\n",
    "\n",
    "df = pd.DataFrame({'id': ids_mat.flatten(), 'unit_sales': preds_mat.flatten()})\n",
    "df['unit_sales'] = df['unit_sales'].map(np.expm1)\n",
    "\n",
    "df[['id', 'unit_sales']].to_csv('sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
